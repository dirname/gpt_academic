# å€Ÿé‰´äº† https://github.com/GaiZhenbiao/ChuanhuChatGPT é¡¹ç›®

"""
    è¯¥æ–‡ä»¶ä¸­ä¸»è¦åŒ…å«ä¸‰ä¸ªå‡½æ•°

    ä¸å…·å¤‡å¤šçº¿ç¨‹èƒ½åŠ›çš„å‡½æ•°ï¼š
    1. predict: æ­£å¸¸å¯¹è¯æ—¶ä½¿ç”¨ï¼Œå…·å¤‡å®Œå¤‡çš„äº¤äº’åŠŸèƒ½ï¼Œä¸å¯å¤šçº¿ç¨‹

    å…·å¤‡å¤šçº¿ç¨‹è°ƒç”¨èƒ½åŠ›çš„å‡½æ•°
    2. predict_no_uiï¼šé«˜çº§å®éªŒæ€§åŠŸèƒ½æ¨¡å—è°ƒç”¨ï¼Œä¸ä¼šå®æ—¶æ˜¾ç¤ºåœ¨ç•Œé¢ä¸Šï¼Œå‚æ•°ç®€å•ï¼Œå¯ä»¥å¤šçº¿ç¨‹å¹¶è¡Œï¼Œæ–¹ä¾¿å®ç°å¤æ‚çš„åŠŸèƒ½é€»è¾‘
    3. predict_no_ui_long_connectionï¼šåœ¨å®éªŒè¿‡ç¨‹ä¸­å‘ç°è°ƒç”¨predict_no_uiå¤„ç†é•¿æ–‡æ¡£æ—¶ï¼Œå’Œopenaiçš„è¿æ¥å®¹æ˜“æ–­æ‰ï¼Œè¿™ä¸ªå‡½æ•°ç”¨streamçš„æ–¹å¼è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒåŒæ ·æ”¯æŒå¤šçº¿ç¨‹
"""

import json
import time
import gradio as gr
import logging
import traceback
import requests
import importlib

# config_private.pyæ”¾è‡ªå·±çš„ç§˜å¯†å¦‚APIå’Œä»£ç†ç½‘å€
# è¯»å–æ—¶é¦–å…ˆçœ‹æ˜¯å¦å­˜åœ¨ç§å¯†çš„config_privateé…ç½®æ–‡ä»¶ï¼ˆä¸å—gitç®¡æ§ï¼‰ï¼Œå¦‚æœæœ‰ï¼Œåˆ™è¦†ç›–åŸconfigæ–‡ä»¶
from toolbox import get_conf, update_ui, is_any_api_key, select_api_key, what_keys, clip_history, trimmed_format_exc, is_the_upload_folder
proxies, TIMEOUT_SECONDS, MAX_RETRY, API_ORG = \
    get_conf('proxies', 'TIMEOUT_SECONDS', 'MAX_RETRY', 'API_ORG')

timeout_bot_msg = '[Local Message] Request timeout. Network error. Please check proxy settings in config.py.' + \
                  'ç½‘ç»œé”™è¯¯ï¼Œæ£€æŸ¥ä»£ç†æœåŠ¡å™¨æ˜¯å¦å¯ç”¨ï¼Œä»¥åŠä»£ç†è®¾ç½®çš„æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼Œæ ¼å¼é¡»æ˜¯[åè®®]://[åœ°å€]:[ç«¯å£]ï¼Œç¼ºä¸€ä¸å¯ã€‚'

def get_full_error(chunk, stream_response):
    """
        è·å–å®Œæ•´çš„ä»Openaiè¿”å›çš„æŠ¥é”™
    """
    while True:
        try:
            chunk += next(stream_response)
        except:
            break
    return chunk


def predict_no_ui_long_connection(inputs, llm_kwargs, history=[], sys_prompt="", observe_window=None, console_slience=False):
    """
    å‘é€è‡³chatGPTï¼Œç­‰å¾…å›å¤ï¼Œä¸€æ¬¡æ€§å®Œæˆï¼Œä¸æ˜¾ç¤ºä¸­é—´è¿‡ç¨‹ã€‚ä½†å†…éƒ¨ç”¨streamçš„æ–¹æ³•é¿å…ä¸­é€”ç½‘çº¿è¢«æã€‚
    inputsï¼š
        æ˜¯æœ¬æ¬¡é—®è¯¢çš„è¾“å…¥
    sys_prompt:
        ç³»ç»Ÿé™é»˜prompt
    llm_kwargsï¼š
        chatGPTçš„å†…éƒ¨è°ƒä¼˜å‚æ•°
    historyï¼š
        æ˜¯ä¹‹å‰çš„å¯¹è¯åˆ—è¡¨
    observe_window = Noneï¼š
        ç”¨äºè´Ÿè´£è·¨è¶Šçº¿ç¨‹ä¼ é€’å·²ç»è¾“å‡ºçš„éƒ¨åˆ†ï¼Œå¤§éƒ¨åˆ†æ—¶å€™ä»…ä»…ä¸ºäº†fancyçš„è§†è§‰æ•ˆæœï¼Œç•™ç©ºå³å¯ã€‚observe_window[0]ï¼šè§‚æµ‹çª—ã€‚observe_window[1]ï¼šçœ‹é—¨ç‹—
    """
    watch_dog_patience = 5 # çœ‹é—¨ç‹—çš„è€å¿ƒ, è®¾ç½®5ç§’å³å¯
    headers, payload = generate_payload(inputs, llm_kwargs, history, system_prompt=sys_prompt, stream=True)
    retry = 0
    while True:
        try:
            # make a POST request to the API endpoint, stream=False
            from .bridge_all import model_info
            endpoint = model_info[llm_kwargs['llm_model']]['endpoint']
            response = requests.post(endpoint, headers=headers, proxies=proxies,
                                    json=payload, stream=True, timeout=TIMEOUT_SECONDS); break
        except requests.exceptions.ReadTimeout as e:
            retry += 1
            traceback.print_exc()
            if retry > MAX_RETRY: raise TimeoutError
            if MAX_RETRY!=0: print(f'è¯·æ±‚è¶…æ—¶ï¼Œæ­£åœ¨é‡è¯• ({retry}/{MAX_RETRY}) â€¦â€¦')

    stream_response =  response.iter_lines()
    result = ''
    json_data = None
    while True:
        try: chunk = next(stream_response).decode()
        except StopIteration: 
            break
        except requests.exceptions.ConnectionError:
            chunk = next(stream_response).decode() # å¤±è´¥äº†ï¼Œé‡è¯•ä¸€æ¬¡ï¼Ÿå†å¤±è´¥å°±æ²¡åŠæ³•äº†ã€‚
        if len(chunk)==0: continue
        if not chunk.startswith('data:'): 
            error_msg = get_full_error(chunk.encode('utf8'), stream_response).decode()
            if "reduce the length" in error_msg:
                raise ConnectionAbortedError("OpenAIæ‹’ç»äº†è¯·æ±‚:" + error_msg)
            else:
                raise RuntimeError("OpenAIæ‹’ç»äº†è¯·æ±‚ï¼š" + error_msg)
        if ('data: [DONE]' in chunk): break # api2d æ­£å¸¸å®Œæˆ
        json_data = json.loads(chunk.lstrip('data:'))['choices'][0]
        delta = json_data["delta"]
        if len(delta) == 0: break
        if "role" in delta: continue
        if "content" in delta:
            result += delta["content"]
            if not console_slience: print(delta["content"], end='')
            if observe_window is not None: 
                # è§‚æµ‹çª—ï¼ŒæŠŠå·²ç»è·å–çš„æ•°æ®æ˜¾ç¤ºå‡ºå»
                if len(observe_window) >= 1:
                    observe_window[0] += delta["content"]
                # çœ‹é—¨ç‹—ï¼Œå¦‚æœè¶…è¿‡æœŸé™æ²¡æœ‰å–‚ç‹—ï¼Œåˆ™ç»ˆæ­¢
                if len(observe_window) >= 2:
                    if (time.time()-observe_window[1]) > watch_dog_patience:
                        raise RuntimeError("ç”¨æˆ·å–æ¶ˆäº†ç¨‹åºã€‚")
        else: raise RuntimeError("æ„å¤–Jsonç»“æ„ï¼š"+delta)
    if json_data and json_data['finish_reason'] == 'content_filter':
        raise RuntimeError("ç”±äºæé—®å«ä¸åˆè§„å†…å®¹è¢«Azureè¿‡æ»¤ã€‚")
    if json_data and json_data['finish_reason'] == 'length':
        raise ConnectionAbortedError("æ­£å¸¸ç»“æŸï¼Œä½†æ˜¾ç¤ºTokenä¸è¶³ï¼Œå¯¼è‡´è¾“å‡ºä¸å®Œæ•´ï¼Œè¯·å‰Šå‡å•æ¬¡è¾“å…¥çš„æ–‡æœ¬é‡ã€‚")
    return result


def predict(inputs, llm_kwargs, plugin_kwargs, chatbot, history=[], system_prompt='', stream = True, additional_fn=None):
    """
    å‘é€è‡³chatGPTï¼Œæµå¼è·å–è¾“å‡ºã€‚
    ç”¨äºåŸºç¡€çš„å¯¹è¯åŠŸèƒ½ã€‚
    inputs æ˜¯æœ¬æ¬¡é—®è¯¢çš„è¾“å…¥
    top_p, temperatureæ˜¯chatGPTçš„å†…éƒ¨è°ƒä¼˜å‚æ•°
    history æ˜¯ä¹‹å‰çš„å¯¹è¯åˆ—è¡¨ï¼ˆæ³¨æ„æ— è®ºæ˜¯inputsè¿˜æ˜¯historyï¼Œå†…å®¹å¤ªé•¿äº†éƒ½ä¼šè§¦å‘tokenæ•°é‡æº¢å‡ºçš„é”™è¯¯ï¼‰
    chatbot ä¸ºWebUIä¸­æ˜¾ç¤ºçš„å¯¹è¯åˆ—è¡¨ï¼Œä¿®æ”¹å®ƒï¼Œç„¶åyeildå‡ºå»ï¼Œå¯ä»¥ç›´æ¥ä¿®æ”¹å¯¹è¯ç•Œé¢å†…å®¹
    additional_fnä»£è¡¨ç‚¹å‡»çš„å“ªä¸ªæŒ‰é’®ï¼ŒæŒ‰é’®è§functional.py
    """
    if is_any_api_key(inputs):
        chatbot._cookies['api_key'] = inputs
        chatbot.append(("è¾“å…¥å·²è¯†åˆ«ä¸º PuerHub AI çš„ä»¤ç‰Œ", what_keys(inputs)))
        yield from update_ui(chatbot=chatbot, history=history, msg="PuerHub AI ä»¤ç‰Œå·²å¯¼å…¥") # åˆ·æ–°ç•Œé¢
        return
    elif not is_any_api_key(chatbot._cookies['api_key']):
        chatbot.append((inputs, """
**ä»…** å¯ **[PuerHub AI](https://ai.puerhub.xyz)** ç”Ÿæˆçš„ä»¤ç‰Œè¿›è¡Œä½¿ç”¨ !ğŸ‘‰ [ç‚¹å‡»è¿™é‡Œ](https://ai.puerhub.xyz/token) ç”Ÿæˆä»¤ç‰Œ ğŸ”‘!

**æ¨è**ğŸ‘ä½¿ç”¨ **æœ¬åœ°ç‰ˆ** è¿è¡Œ, **ç‹¬äº«æœ¬åœ°è®¡ç®—èµ„æº(æ›´å¿«å¤„ç†æ–‡ä»¶)**, **æ— éœ€æ¯æ¬¡è¾“å…¥ä»¤ç‰Œ**, ä¸ºä½ æä¾›æ›´æµç•…å®‰å…¨çš„ä½“éªŒ !ğŸš€[ç‚¹å‡»è¿™é‡Œ](https://puerhub.yuque.com/org-wiki-vtcqi0/fuxcn8/vi4uegpwm99ur4c7#afn8U) æŸ¥çœ‹æœ¬åœ°ç‰ˆè¿è¡Œæ‰‹å†ŒğŸ“– !
        """))
        yield from update_ui(chatbot=chatbot, history=history, msg="ç¼ºå°‘api_key") # åˆ·æ–°ç•Œé¢
        return

    user_input = inputs
    if additional_fn is not None:
        from core_functional import handle_core_functionality
        inputs, history = handle_core_functionality(additional_fn, inputs, history, chatbot)

    raw_input = inputs
    logging.info(f'[raw_input] {raw_input}')
    chatbot.append((inputs, ""))
    yield from update_ui(chatbot=chatbot, history=history, msg="ç­‰å¾…å“åº”") # åˆ·æ–°ç•Œé¢

    # check mis-behavior
    if is_the_upload_folder(user_input):
        chatbot[-1] = (inputs, f"[Local Message] æ£€æµ‹åˆ°æ“ä½œé”™è¯¯ï¼å½“æ‚¨ä¸Šä¼ æ–‡æ¡£ä¹‹åï¼Œéœ€ç‚¹å‡»â€œ**å‡½æ•°æ’ä»¶åŒº**â€æŒ‰é’®è¿›è¡Œå¤„ç†ï¼Œè¯·å‹¿ç‚¹å‡»â€œæäº¤â€æŒ‰é’®æˆ–è€…â€œåŸºç¡€åŠŸèƒ½åŒºâ€æŒ‰é’®ã€‚")
        yield from update_ui(chatbot=chatbot, history=history, msg="æ­£å¸¸") # åˆ·æ–°ç•Œé¢
        time.sleep(2)

    try:
        headers, payload = generate_payload(inputs, llm_kwargs, history, system_prompt, stream)
    except RuntimeError as e:
        chatbot[-1] = (inputs, f"æ‚¨æä¾›çš„api-keyä¸æ»¡è¶³è¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•å¯ç”¨äº{llm_kwargs['llm_model']}çš„api-keyã€‚æ‚¨å¯èƒ½é€‰æ‹©äº†é”™è¯¯çš„æ¨¡å‹æˆ–è¯·æ±‚æºã€‚")
        yield from update_ui(chatbot=chatbot, history=history, msg="api-keyä¸æ»¡è¶³è¦æ±‚") # åˆ·æ–°ç•Œé¢
        return
        
    history.append(inputs); history.append("")

    retry = 0
    while True:
        try:
            # make a POST request to the API endpoint, stream=True
            from .bridge_all import model_info
            endpoint = model_info[llm_kwargs['llm_model']]['endpoint']
            response = requests.post(endpoint, headers=headers, proxies=proxies,
                                    json=payload, stream=True, timeout=TIMEOUT_SECONDS);break
        except:
            retry += 1
            chatbot[-1] = ((chatbot[-1][0], timeout_bot_msg))
            retry_msg = f"ï¼Œæ­£åœ¨é‡è¯• ({retry}/{MAX_RETRY}) â€¦â€¦" if MAX_RETRY > 0 else ""
            yield from update_ui(chatbot=chatbot, history=history, msg="è¯·æ±‚è¶…æ—¶"+retry_msg) # åˆ·æ–°ç•Œé¢
            if retry > MAX_RETRY: raise TimeoutError

    gpt_replying_buffer = ""
    
    is_head_of_the_stream = True
    if stream:
        stream_response =  response.iter_lines()
        while True:
            try:
                chunk = next(stream_response)
            except StopIteration:
                # éOpenAIå®˜æ–¹æ¥å£çš„å‡ºç°è¿™æ ·çš„æŠ¥é”™ï¼ŒOpenAIå’ŒAPI2Dä¸ä¼šèµ°è¿™é‡Œ
                chunk_decoded = chunk.decode()
                error_msg = chunk_decoded
                # é¦–å…ˆæ’é™¤ä¸€ä¸ªone-apiæ²¡æœ‰doneæ•°æ®åŒ…çš„ç¬¬ä¸‰æ–¹Bugæƒ…å½¢
                if len(gpt_replying_buffer.strip()) > 0 and len(error_msg) == 0: 
                    yield from update_ui(chatbot=chatbot, history=history, msg="æ£€æµ‹åˆ°æœ‰ç¼ºé™·çš„éOpenAIå®˜æ–¹æ¥å£ï¼Œå»ºè®®é€‰æ‹©æ›´ç¨³å®šçš„æ¥å£ã€‚")
                    break
                # å…¶ä»–æƒ…å†µï¼Œç›´æ¥è¿”å›æŠ¥é”™
                chatbot, history = handle_error(inputs, llm_kwargs, chatbot, history, chunk_decoded, error_msg)
                yield from update_ui(chatbot=chatbot, history=history, msg="éOpenAIå®˜æ–¹æ¥å£è¿”å›äº†é”™è¯¯:" + chunk.decode()) # åˆ·æ–°ç•Œé¢
                return
            
            chunk_decoded = chunk.decode()
            if is_head_of_the_stream and (r'"object":"error"' not in chunk_decoded) and (r"content" not in chunk_decoded):
                # æ•°æ®æµçš„ç¬¬ä¸€å¸§ä¸æºå¸¦content
                is_head_of_the_stream = False; continue
            
            if chunk:
                try:
                    # å‰è€…æ˜¯API2Dçš„ç»“æŸæ¡ä»¶ï¼Œåè€…æ˜¯OPENAIçš„ç»“æŸæ¡ä»¶
                    if ('data: [DONE]' in chunk_decoded) or (len(json.loads(chunk_decoded[6:])['choices'][0]["delta"]) == 0):
                        # åˆ¤å®šä¸ºæ•°æ®æµçš„ç»“æŸï¼Œgpt_replying_bufferä¹Ÿå†™å®Œäº†
                        logging.info(f'[response] {gpt_replying_buffer}')
                        break
                    # å¤„ç†æ•°æ®æµçš„ä¸»ä½“
                    chunkjson = json.loads(chunk_decoded[6:])
                    status_text = f"finish_reason: {chunkjson['choices'][0].get('finish_reason', 'null')}"
                    # å¦‚æœè¿™é‡ŒæŠ›å‡ºå¼‚å¸¸ï¼Œä¸€èˆ¬æ˜¯æ–‡æœ¬è¿‡é•¿ï¼Œè¯¦æƒ…è§get_full_errorçš„è¾“å‡º
                    gpt_replying_buffer = gpt_replying_buffer + chunkjson['choices'][0]["delta"]["content"]
                    history[-1] = gpt_replying_buffer
                    chatbot[-1] = (history[-2], history[-1])
                    yield from update_ui(chatbot=chatbot, history=history, msg=status_text) # åˆ·æ–°ç•Œé¢
                except Exception as e:
                    yield from update_ui(chatbot=chatbot, history=history, msg="Jsonè§£æä¸åˆå¸¸è§„") # åˆ·æ–°ç•Œé¢
                    chunk = get_full_error(chunk, stream_response)
                    chunk_decoded = chunk.decode()
                    error_msg = chunk_decoded
                    chatbot, history = handle_error(inputs, llm_kwargs, chatbot, history, chunk_decoded, error_msg)
                    yield from update_ui(chatbot=chatbot, history=history, msg="Jsonå¼‚å¸¸" + error_msg) # åˆ·æ–°ç•Œé¢
                    print(error_msg)
                    return

def handle_error(inputs, llm_kwargs, chatbot, history, chunk_decoded, error_msg):
    from .bridge_all import model_info
    openai_website = ' è¯·ç™»å½•OpenAIæŸ¥çœ‹è¯¦æƒ… https://platform.openai.com/signup'
    if "reduce the length" in error_msg:
        if len(history) >= 2: history[-1] = ""; history[-2] = "" # æ¸…é™¤å½“å‰æº¢å‡ºçš„è¾“å…¥ï¼šhistory[-2] æ˜¯æœ¬æ¬¡è¾“å…¥, history[-1] æ˜¯æœ¬æ¬¡è¾“å‡º
        history = clip_history(inputs=inputs, history=history, tokenizer=model_info[llm_kwargs['llm_model']]['tokenizer'], 
                                               max_token_limit=(model_info[llm_kwargs['llm_model']]['max_token'])) # historyè‡³å°‘é‡Šæ”¾äºŒåˆ†ä¹‹ä¸€
        chatbot[-1] = (chatbot[-1][0], "[Local Message] Reduce the length. æœ¬æ¬¡è¾“å…¥è¿‡é•¿, æˆ–å†å²æ•°æ®è¿‡é•¿. å†å²ç¼“å­˜æ•°æ®å·²éƒ¨åˆ†é‡Šæ”¾, æ‚¨å¯ä»¥è¯·å†æ¬¡å°è¯•. (è‹¥å†æ¬¡å¤±è´¥åˆ™æ›´å¯èƒ½æ˜¯å› ä¸ºè¾“å…¥è¿‡é•¿.)")
    elif "does not exist" in error_msg:
        chatbot[-1] = (chatbot[-1][0], f"[Local Message] Model {llm_kwargs['llm_model']} does not exist. æ¨¡å‹ä¸å­˜åœ¨, æˆ–è€…æ‚¨æ²¡æœ‰è·å¾—ä½“éªŒèµ„æ ¼.")
    elif "Incorrect API key" in error_msg:
        chatbot[-1] = (chatbot[-1][0], "[Local Message] Incorrect API key. OpenAIä»¥æä¾›äº†ä¸æ­£ç¡®çš„API_KEYä¸ºç”±, æ‹’ç»æœåŠ¡. " + openai_website)
    elif "exceeded your current quota" in error_msg:
        chatbot[-1] = (chatbot[-1][0], "[Local Message] You exceeded your current quota. OpenAIä»¥è´¦æˆ·é¢åº¦ä¸è¶³ä¸ºç”±, æ‹’ç»æœåŠ¡." + openai_website)
    elif "account is not active" in error_msg:
        chatbot[-1] = (chatbot[-1][0], "[Local Message] Your account is not active. OpenAIä»¥è´¦æˆ·å¤±æ•ˆä¸ºç”±, æ‹’ç»æœåŠ¡." + openai_website)
    elif "associated with a deactivated account" in error_msg:
        chatbot[-1] = (chatbot[-1][0], "[Local Message] You are associated with a deactivated account. OpenAIä»¥è´¦æˆ·å¤±æ•ˆä¸ºç”±, æ‹’ç»æœåŠ¡." + openai_website)
    elif "bad forward key" in error_msg:
        chatbot[-1] = (chatbot[-1][0], "[Local Message] Bad forward key. API2Dè´¦æˆ·é¢åº¦ä¸è¶³.")
    elif "Not enough point" in error_msg:
        chatbot[-1] = (chatbot[-1][0], "[Local Message] Not enough point. API2Dè´¦æˆ·ç‚¹æ•°ä¸è¶³.")
    else:
        from toolbox import regular_txt_to_markdown
        tb_str = '```\n' + trimmed_format_exc() + '```'
        chatbot[-1] = (chatbot[-1][0], f"[Local Message] å¼‚å¸¸ \n\n{tb_str} \n\n{regular_txt_to_markdown(chunk_decoded)}")
    return chatbot, history

def generate_payload(inputs, llm_kwargs, history, system_prompt, stream):
    """
    æ•´åˆæ‰€æœ‰ä¿¡æ¯ï¼Œé€‰æ‹©LLMæ¨¡å‹ï¼Œç”Ÿæˆhttpè¯·æ±‚ï¼Œä¸ºå‘é€è¯·æ±‚åšå‡†å¤‡
    """
    if not is_any_api_key(llm_kwargs['api_key']):
        raise AssertionError("ä½ æä¾›äº†é”™è¯¯çš„API_KEYã€‚\n\n1. ä¸´æ—¶è§£å†³æ–¹æ¡ˆï¼šç›´æ¥åœ¨è¾“å…¥åŒºé”®å…¥api_keyï¼Œç„¶åå›è½¦æäº¤ã€‚\n\n2. é•¿æ•ˆè§£å†³æ–¹æ¡ˆï¼šåœ¨config.pyä¸­é…ç½®ã€‚")

    api_key = select_api_key(llm_kwargs['api_key'], llm_kwargs['llm_model'])

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    if API_ORG.startswith('org-'): headers.update({"OpenAI-Organization": API_ORG})
    if llm_kwargs['llm_model'].startswith('azure-'): headers.update({"api-key": api_key})

    conversation_cnt = len(history) // 2

    messages = [{"role": "system", "content": system_prompt}]
    if conversation_cnt:
        for index in range(0, 2*conversation_cnt, 2):
            what_i_have_asked = {}
            what_i_have_asked["role"] = "user"
            what_i_have_asked["content"] = history[index]
            what_gpt_answer = {}
            what_gpt_answer["role"] = "assistant"
            what_gpt_answer["content"] = history[index+1]
            if what_i_have_asked["content"] != "":
                if what_gpt_answer["content"] == "": continue
                if what_gpt_answer["content"] == timeout_bot_msg: continue
                messages.append(what_i_have_asked)
                messages.append(what_gpt_answer)
            else:
                messages[-1]['content'] = what_gpt_answer['content']

    what_i_ask_now = {}
    what_i_ask_now["role"] = "user"
    what_i_ask_now["content"] = inputs
    messages.append(what_i_ask_now)

    if str(llm_kwargs['llm_model']).startswith('api2d'):
        llm_kwargs['llm_model'] = llm_kwargs['llm_model'][5:]

    payload = {
        "model": llm_kwargs['llm_model'],
        "messages": messages, 
        "temperature": llm_kwargs['temperature'],  # 1.0,
        "top_p": llm_kwargs['top_p'],  # 1.0,
        "n": 1,
        "stream": stream,
        "presence_penalty": 0,
        "frequency_penalty": 0,
    }
    try:
        print(f" {llm_kwargs['llm_model']} : {conversation_cnt} : {inputs[:100]} ..........")
    except:
        print('è¾“å…¥ä¸­å¯èƒ½å­˜åœ¨ä¹±ç ã€‚')
    return headers,payload


